## 마스터 노드 기본 환경 구성 설명서
# 쿠버네티스는 최소한 v1.22 이상의 버전이 필요하다.
# Ansible의 명령어를 실행하기 위해 Ansible v2.11+, Jinja 2.11+와 Python netaddr 라이브러리가 머신에 설치되어 있어야 한다.

# epel-release 설치 및 ansible 설치
yum install -y epel-release
yum update -y
yum install -y ansible
yum install -y python3 libselinux-python3

# python 2 -> 3로 업그레이드
yum install -y python3

# python2에 관한 심볼릭 링크 모두 제거
rm /usr/bin/python && rm /usr/bin/python2

ln -s /usr/bin/python3 /usr/bin/python
ln -s /usr/bin/python2.7 /usr/bin/python2

# example 이런식으로 세팅됨
/usr/bin/python -> /usr/bin/python3
/usr/bin/python3 -> /usr/bin/python3.6
/usr/bin/python2 -> /usr/bin/python2.7

# yum 패키지 경로 변경

yum은 python2 기반 패키지

vi /usr/bin/yum
맨위를 /usr/bin/pyhon2로 변경해준다.

vi /usr/libexec/urlgrabber-ext-down
맨위를 /usr/bin/pyhon2로 변경해준다.

# 노드간 SSH 키교환
ssh-keygen -t rsa
ssh-copy root@[노드 IP] -i ./.ssh/id_rsa.pub

# /etc/hosts 설정
echo "192.168.88.10 master
192.168.88.20 node1
192.168.88.30 node2" >>/etc/hosts

# SELINUX 설정
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux

# 컨테이너 내부 패킷 제어 설정
echo "	net.bridge.bridge-nf-call-iptables = 1
	net.bridge.bridge-nf-call-ip6tables = 1
	net.ipv4.ip_forward = 1 " >>/etc/sysctl.conf

# SWAP 메모리 비활성화
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
swapoff -a

# git clone kubespray 
git clone https://github.com/kubernetes-sigs/kubespray.git

# kubespray가 요구하는 패키지 설치
cd kubespray
git checkout release-2.15
pip3 install -r requirements.txt
오류 발생 시 
python3.6 -m pip install --upgrade pip

# 나만의 inventory 만들기
cp -rfp inventory/sample inventory/mycluster
vi ./inventory/mycluster/inventory.ini

[all]
master        ansible_host=192.168.88.10       ip=192.168.88.10
node1         ansible_host=192.168.88.20       ip=192.168.88.20
node2         ansible_host=192.168.88.30       ip=192.168.88.30

[kube-master]
master

[etcd]
master

[kube-node]
node1
node2

[k8s-cluster:children]
kube_control_plane
kube_node
calico_rr

# Playbook 실행
ansible-playbook -i inventory/mycluster/inventory.ini \
-e ansible_python_interpreter=/usr/bin/python2 -become --become-user=root cluster.yml 


